{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T12:31:27.235973Z",
     "start_time": "2020-04-16T12:31:25.808658Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import logit\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from keras.layers import (Input, Dense, Lambda, Flatten, Reshape, BatchNormalization, Layer,\n",
    "                          Activation, Dropout, Conv2D, Conv2DTranspose,\n",
    "                          Concatenate, add, Add, Multiply)\n",
    "from keras.engine import InputSpec\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from realnvp_helpers import Mask, FlowBatchNorm\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T12:31:27.244024Z",
     "start_time": "2020-04-16T12:31:27.237983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4, 4, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 5.4794617 ,  1.28151635,  3.75917387],\n",
       "        [ 0.36384094, -5.07506591, -3.08312326],\n",
       "        [-2.59537507,  4.17044078, -1.52985607],\n",
       "        [ 2.08610113,  3.4017493 , -3.58502542]],\n",
       "\n",
       "       [[ 1.20376355, -4.24876297, -1.92980602],\n",
       "        [ 1.17610967, -1.52501444, -6.34207611],\n",
       "        [ 3.00506666,  2.03983978, -1.89792492],\n",
       "        [-1.10339436, -2.93596685, -3.11919329]],\n",
       "\n",
       "       [[ 4.84954729,  3.23292433,  0.85169632],\n",
       "        [ 0.62558932, -0.68053771, -0.27105693],\n",
       "        [ 0.09714588, -1.53127008,  1.77989377],\n",
       "        [ 3.91475617,  5.85430099,  3.35543008]],\n",
       "\n",
       "       [[-2.5338356 , -2.39656266, -0.8864732 ],\n",
       "        [ 0.11184573,  2.73034737,  3.28751866],\n",
       "        [ 0.83016145,  0.19297795,  2.55395453],\n",
       "        [-2.34938273, -3.42326378, -2.40890654]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "shape = (4, 4, 3)\n",
    "samples = 100\n",
    "\n",
    "train_data = np.random.normal(0.5, 3, size=(samples,) + (shape))\n",
    "print(train_data.shape)\n",
    "train_data[0, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T12:31:27.334281Z",
     "start_time": "2020-04-16T12:31:27.245687Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv_block(input_shape, kernel_size, filters, stage, block, use_resid=True):\n",
    "    ''' Adapted from resnet50 implementation in Keras '''\n",
    "    filters1, filters2, filters3 = filters\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    x = Conv2D(filters1, (1, 1),\n",
    "               kernel_initializer='he_normal',\n",
    "               name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size,\n",
    "               padding='same',\n",
    "               kernel_initializer='he_normal',\n",
    "               name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1),\n",
    "               kernel_initializer='he_normal',\n",
    "               name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    if use_resid:\n",
    "        x = add([x, input_tensor])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return Model(input_tensor, x, name='conv_block' + stage + block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T12:31:27.466321Z",
     "start_time": "2020-04-16T12:31:27.337060Z"
    }
   },
   "outputs": [],
   "source": [
    "def coupling_layer(input_shape, mask_type, stage):\n",
    "    ''' Implements (as per paper):\n",
    "        y = b * x + (1 - b) * [x * exp(s(b * x)) + t(b * x)]\n",
    "    '''\n",
    "    assert mask_type in ['check_even', 'check_odd', 'channel_even', 'channel_odd']\n",
    "    mask_prefix = 'check' if mask_type.startswith('check') else 'channel'\n",
    "    mask_opposite = 'odd' if mask_type.endswith('even') else 'even'\n",
    "    \n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    \n",
    "    # Raw operations for step\n",
    "    b0 = Mask(mask_type)\n",
    "    b1 = Mask(mask_prefix + '_' + mask_opposite)\n",
    "    s_ = conv_block(input_shape, (3, 3), (32, 32, 3), stage, '_s', use_resid=True)\n",
    "    t_ = conv_block(input_shape, (3, 3), (32, 32, 3), stage, '_t', use_resid=True)\n",
    "    batch = FlowBatchNorm()\n",
    "       \n",
    "    # Forward\n",
    "    masked_input = b1(input_tensor)\n",
    "    s = s_(masked_input)\n",
    "    t = t_(masked_input)\n",
    "    coupling = Lambda(lambda ins:  ins[0] * K.exp(ins[1]) + ins[2])([input_tensor, s, t])\n",
    "    coupling_mask = b0(coupling)\n",
    "    out1, out2 = Add()([masked_input, coupling_mask]), b0(s)\n",
    "    out1_norm, gamma, var = batch(out1)\n",
    "    #batch_loss = Lambda(lambda x: - (K.log(gamma) - 0.5 * K.log(x + batch.epsilon)))(var)\n",
    "    batch_loss = Lambda(lambda x: -K.log(gamma))(var)\n",
    "    #batch_loss = Lambda(lambda x: - ( - 0.5 * K.log(x + batch.epsilon)))(var)\n",
    "    \n",
    "    # Reverse\n",
    "   \n",
    "    # Return result + masked scale for loss function\n",
    "    return Model(input_tensor, [out1_norm, out2, batch_loss], name='_'.join(['coupling', mask_type, stage]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T12:31:27.583156Z",
     "start_time": "2020-04-16T12:31:27.469507Z"
    }
   },
   "outputs": [],
   "source": [
    "def coupling_group(input_tensor, steps, mask_type, stage):\n",
    "    name_mapping = dict(enumerate(string.ascii_lowercase))\n",
    "    \n",
    "    # TODO: Only need check/channel, not even/odd right?\n",
    "    assert mask_type in ['check_even', 'check_odd', 'channel_even', 'channel_odd']\n",
    "    mask_prefix = 'check' if mask_type.startswith('check') else 'channel'\n",
    "    \n",
    "    input_shape = tuple(x.value for x in input_tensor.shape)[1:]\n",
    "    x = input_tensor\n",
    "    s_losses = []\n",
    "    batch_losses = []\n",
    "    for i in range(3):\n",
    "        mask_type = mask_prefix + ('_even' if i % 2 == 0 else '_odd')\n",
    "        step = coupling_layer(input_shape, mask_type, stage=str(stage) + name_mapping[i])\n",
    "        x, s, batch_loss = step(x)\n",
    "        #x, s = step(x)\n",
    "        s_losses.append(s)\n",
    "        batch_losses.append(batch_loss)\n",
    "    \n",
    "    return x, s_losses, batch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T12:31:27.701075Z",
     "start_time": "2020-04-16T12:31:27.585781Z"
    }
   },
   "outputs": [],
   "source": [
    "def realnvp_zloss(target, z):\n",
    "    # log(p_X(x)) = log(p_Z(f(x))) + log(|det(\\partial f(x) / \\partial X^T)|)\n",
    "    # Prior is standard normal(mu=0, sigma=1)\n",
    "    shape = z.shape\n",
    "    return K.sum(-0.5 * np.log(math.pi) - 0.5 * z**2, axis=list(range(1, len(shape[1:]))))\n",
    "\n",
    "def const_loss(target, output):\n",
    "    # For debugging\n",
    "    return K.constant(0)\n",
    "\n",
    "def realnvp_sumloss(target, output):\n",
    "    # Determinant is just sum of \"s\" or \"batch loss\" params (already log-space)\n",
    "    shape = output.shape\n",
    "    return K.sum(output, axis=list(range(1, len(shape))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T12:31:32.539776Z",
     "start_time": "2020-04-16T12:31:27.703677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 4, 4, 3)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "coupling_check_even_1a (Model)  [(None, 4, 4, 3), (N 19498       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "coupling_check_odd_1b (Model)   [(None, 4, 4, 3), (N 19498       coupling_check_even_1a[1][0]     \n",
      "__________________________________________________________________________________________________\n",
      "coupling_check_even_1c (Model)  [(None, 4, 4, 3), (N 19498       coupling_check_odd_1b[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "s_losses (Concatenate)          (None, 4, 4, 9)      0           coupling_check_even_1a[1][1]     \n",
      "                                                                 coupling_check_odd_1b[1][1]      \n",
      "                                                                 coupling_check_even_1c[1][1]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_losses (Concatenate)      (None, 4, 4, 9)      0           coupling_check_even_1a[1][2]     \n",
      "                                                                 coupling_check_odd_1b[1][2]      \n",
      "                                                                 coupling_check_even_1c[1][2]     \n",
      "==================================================================================================\n",
      "Total params: 58,494\n",
      "Trainable params: 57,672\n",
      "Non-trainable params: 822\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_tensor = Input(shape=shape)\n",
    "#x = conv_block(shape, (3, 3), (32, 32, 3), '0', '_s', use_resid=True)(input_tensor)\n",
    "#step = coupling_layer(shape, 'check_even', stage=str('a') + '0')\n",
    "#x, s, batch_loss = step(input_tensor)\n",
    "#s_losses = [s, s]\n",
    "#batch_losses = [batch_loss, batch_loss]\n",
    "\n",
    "x, s_losses, batch_losses = coupling_group(input_tensor, steps=3, mask_type='check_even', stage=1)\n",
    "s_losses = Concatenate(name='s_losses')(s_losses)\n",
    "batch_losses = Concatenate(name='batch_losses')(batch_losses)\n",
    "\n",
    "forward_model = Model(inputs=input_tensor, outputs=[x, s_losses, batch_losses])\n",
    "optimizer = Adam(lr=0.001)\n",
    "forward_model.compile(optimizer=optimizer, \n",
    "                      #loss=[realnvp_zloss, realnvp_sumloss, realnvp_sumloss])\n",
    "                      loss=[const_loss, const_loss, realnvp_sumloss])\n",
    "forward_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T12:31:37.000953Z",
     "start_time": "2020-04-16T12:31:32.541891Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "An operation has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-249efcecca7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTQDMNotebookCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#, tensorboard], #, early_stopping, reduce_lr],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    507\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[1;32m    508\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                         loss=self.total_loss)\n\u001b[0m\u001b[1;32m    510\u001b[0m                 updates = (self.updates +\n\u001b[1;32m    511\u001b[0m                            \u001b[0mtraining_updates\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_updates_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_gradients\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             raise ValueError('An operation has `None` for gradient. '\n\u001b[0m\u001b[1;32m     92\u001b[0m                              \u001b[0;34m'Please make sure that all of your ops have a '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                              \u001b[0;34m'gradient defined (i.e. are differentiable). '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: An operation has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval."
     ]
    }
   ],
   "source": [
    "#early_stopping = keras.callbacks.EarlyStopping('val_loss', min_delta=50.0, patience=5)\n",
    "#reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.0001)\n",
    "s = [len(train_data)] + [int(x) for x in s_losses.shape[1:]]\n",
    "#s[0] = int(train_data.shape[0])\n",
    "#print(train_data.shape, np.zeros(s).shape)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='graph', \n",
    "                          batch_size=batch_size, \n",
    "                          histogram_freq=1, \n",
    "                          write_graph=True) \n",
    "history = forward_model.fit(\n",
    "    train_data, [train_data, np.zeros(s), np.zeros(s)],\n",
    "    #validation_data=(train_data[:10], [train_data[:10], np.zeros(s)[:10], np.zeros(s)[:10]]),\n",
    "    batch_size=batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[TQDMNotebookCallback()], #, tensorboard], #, early_stopping, reduce_lr],\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T12:31:37.001746Z",
     "start_time": "2020-04-16T12:31:25.817Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history.history)\n",
    "#display(df.describe(percentiles=[0.25 * i for i in range(4)] + [0.95, 0.99]))\n",
    "col = 'val_loss' if 'val_loss' in df else 'loss'\n",
    "display(df[-25:])\n",
    "df[col][-25:].plot(figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019-07-28\n",
    "\n",
    "* Got some framework up to do coupling layers but having trouble passing the scale parameter to the loss function, getting some weird tensorflow error, needs more debugging\n",
    "* Without the determinant in the loss function, it looks like loss goes down, so maybe on the right track?\n",
    "    * It's actually weird that we're not using the image in the output, but I guess that's what's great about this reversible model!\n",
    "* TODO:\n",
    "    * Debug scale function in loss\n",
    "    * Add reverse (generator) network to functions above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019-07-29\n",
    "\n",
    "* Explanation of how to estimate probability of continuous variables (relevant for computing bits/pixel without an explicit discrete distribution): https://math.stackexchange.com/questions/2818318/probability-that-a-sample-is-generated-from-a-distribution\n",
    "* Idea for a post, explain likelihood estimation of discrete vs. continuous distributions (like pixels), include:\n",
    "  * Probability of observing a value from continuous distribution = 0\n",
    "     * https://math.stackexchange.com/questions/2818318/probability-that-a-sample-is-generated-from-a-distribution\n",
    "  * Probability of observing a value from a set of discrete hypthesis (models) is non-zero using epsilon trick (see above link):\n",
    "     * https://math.stackexchange.com/questions/920241/can-an-observed-event-in-fact-be-of-zero-probability\n",
    "  * Explain Equation 3 from \"A NOTE ON THE EVALUATION OF GENERATIVE MODELS\"\n",
    "     * Also include an example using a simpler case, like a bernoulli variable that we're estimating using a continuous distribution\n",
    "  * Bring it back to modelling pixels and how they usually do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-03-30\n",
    "\n",
    "* To make reversible network, build forward and backward network at the same time using `Model()` to have components that I can use in both networks\n",
    "* Looks like I have some instability here, depending on the run I can get an exact fit (-100s loss) or a poor a fit (+10):\n",
    "    * Turning off residual networks helps\n",
    "    * Adjusting the learning rate, batch size helps but hard to pinpoint a methodology\n",
    "* Most likely it's the instability of using a scale parameter (RealNVP paper Section 3.7), might need to implement their batch norm for more stable results, especially when adding more layers:\n",
    "    * Reimplement `BatchNorm`: https://github.com/keras-team/keras/blob/master/keras/layers/normalization.py\n",
    "    * Except return regular result AND (variance + eps) term\n",
    "    * Use the (var + eps) term to compute Jacobian for loss function (should just be log-additive)\n",
    "* Once this is done, add back the other stuff:\n",
    "    * Turn on residual shortcuts\n",
    "    * Change batch size to reasonable number and learning rate=0.01\n",
    "* If this still doesn't work, might want to implement \"Running average over recent minibatches\" in Appendix E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-03-31\n",
    "\n",
    "* Fixed a bug (I think) in the network where the coupling layer was wrong.  However, it still sometimes get stuck at around a loss of 5 but more often than not (on another training run) get to -10 (after 20 iters).\n",
    "* Trying to get FlowBatchNorm worknig but having some issues passing the determinant batch loss as an output because the `batch_size` is not getting passed (it has dimension (3,) but should have dimension (None, 3)).  Need to figure out how to tranlate a tensor to Layer that includes batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-04-05\n",
    "\n",
    "* Reminder: BatchNormalization on conv layers only need to normalize across [B, W, H, :] layers, not the \"C\" layer because the filter is identical across a channel (so it uses the same mean/var to normalize).  This is nice because it's the same axis (-1) you would normalize across in a Dense layer. See: https://intellipaat.com/community/3872/batch-normalization-in-convolutional-neural-network\n",
    "* I think I figured out how to return the batchnorm weights back but now I'm hitting a roadblock when I try to merge them together to put as part of the output loss -- maybe I should just forget it and use the tensors directly in the output loss?\n",
    "* Now that I switched to an explicit batch size, it doesn't run anymore... get this error \"Incompatible shapes: [4] vs. [32]\", probably some assumption that I had, got to work backwards and fix it I think.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-04-14\n",
    "\n",
    "* Okay figured out the weird error I was getting: when a Keras model has multiple outputs you either have to give it a list or dict of loss functions, otherwise it will apply the same loss to each output!  Of course, I just assumed that it gives you all outputs in one loss function. So silly!\n",
    "* I reverted the change to explicitly set batch. Instead in the `BatchNormFlow` layer I just multiply zero by the `inputs` and then add the mean/variance.  I think this gives the right shape?\n",
    "* **TODOs**:\n",
    "  * Check that shape/computation for `BatchNormFlow`/`batch_losses` loss is correct\n",
    "  * Check that loss functions are actually returning a negative log-loss (not just the log)\n",
    "  * Validate the model is fitting what I want (right now I have an elbow effect as I train more) -- should there be backprop through the batch_losses? I guess not?  Check the paper and figure out what to do.\n",
    "  * Add back in the bigger model that has multiple coupling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-04-15\n",
    "\n",
    "* Somehow I suspect that the batch loss is not getting optimized (the var parameter in the batch norm function).  When I set the other loss components to zero, I see that hte batch loss is not really getting smaller -- should it?\n",
    "\n",
    "        loss \tcoupling_check_even_1c_loss \ts_losses_loss \tbatch_losses_loss\n",
    "        0 \t146.227879 \t0.0 \t0.0 \t146.227879\n",
    "        1 \t131.294226 \t0.0 \t0.0 \t131.294226\n",
    "        2 \t135.579913 \t0.0 \t0.0 \t135.579913\n",
    "        3 \t127.908073 \t0.0 \t0.0 \t127.908073\n",
    "        4 \t130.301921 \t0.0 \t0.0 \t130.301921\n",
    "        5 \t139.414369 \t0.0 \t0.0 \t139.414369\n",
    "        6 \t129.732767 \t0.0 \t0.0 \t129.732767\n",
    "        7 \t127.321448 \t0.0 \t0.0 \t127.321448\n",
    "        8 \t130.812973 \t0.0 \t0.0 \t130.812973\n",
    "        9 \t136.737979 \t0.0 \t0.0 \t136.737979\n",
    "        10 \t135.001893 \t0.0 \t0.0 \t135.001893\n",
    "        11 \t140.181680 \t0.0 \t0.0 \t140.181680\n",
    "        12 \t133.053322 \t0.0 \t0.0 \t133.053322\n",
    "        13 \t132.912917 \t0.0 \t0.0 \t132.912917\n",
    "        14 \t122.261415 \t0.0 \t0.0 \t122.261415\n",
    "        15 \t139.447081 \t0.0 \t0.0 \t139.447081\n",
    "        16 \t134.216364 \t0.0 \t0.0 \t134.216364\n",
    "        17 \t133.567210 \t0.0 \t0.0 \t133.567210\n",
    "        18 \t131.333447 \t0.0 \t0.0 \t131.333447\n",
    "        19 \t133.022141 \t0.0 \t0.0 \t133.022141\n",
    "        \n",
    "* **IDEA:** I should probably unit test the batch norm flow layer to make sure that it's doing what I think it should be doing... need to think about how to structure this experiment.\n",
    "* **CHECK**: Should `s` loss be negated also?  Seems like I need negative log loss, not just log loss..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-04-16\n",
    "\n",
    "* Forgot that BatchNorm has two components:  $\\mu, \\sigma^2$, the mean and variance of the batch, which we scale ($\\hat{x} = \\frac{x-\\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$) AND two learnable parameters: $\\gamma, \\beta$, which are used to scale the output: $y = \\gamma \\hat{x} + \\beta$.  The learnable parameters are the only ones that change!\n",
    "* Now, how does that work when calculating the determinant? Let's see:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial y} \\hat{y} = \\frac{\\partial}{\\partial y}\\big[\\gamma * \\frac{x-\\mu}{\\sqrt{\\sigma^2 + \\epsilon} + \\beta}\\big]$$\n",
    "$$ = \\frac{\\gamma}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n",
    "\n",
    "  Therefore, I need to include gamma in the determinant calculation in the batch norm layer!\n",
    "  \n",
    "  \n",
    "Ohhhhh... use `keras.layer.add_loss()` function instead of passing the new things over!  Not sure how to deal with batch though... https://www.tensorflow.org/guide/keras/custom_layers_and_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T12:31:37.005980Z",
     "start_time": "2020-04-16T12:31:25.820Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "for i in range(-10, 10):\n",
    "    eps = i / 1000\n",
    "    l = norm.cdf(0 - eps)\n",
    "    r = norm.cdf(0 + eps)\n",
    "    print(eps, '\\t', l - r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T12:31:37.007129Z",
     "start_time": "2020-04-16T12:31:25.821Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.array([[[-1, -2], [-3, -4]], [[1,2], [3, 4]], [[5,6], [7, 8]]])  \n",
    "b = np.array([100, 200]).reshape([1, 1, 2])\n",
    "\n",
    "c = a + b\n",
    "c[:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
